{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1472155",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 0. Imports & environment\n",
    "# ==============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.linalg import condition, pinv\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee93ec5d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 1. Load a public proxy dataset (UCI Thyroid Cancer Recurrence)\n",
    "# ==============================================================\n",
    "# Download from: https://archive.ics.uci.edu/ml/machine-learning-databases/00615/\n",
    "# File name: thyroid_recurrence.csv (383 rows, 12 features + target)\n",
    "df = pd.read_csv('thyroid_recurrence.csv')\n",
    "print(f\"Raw shape: {df.shape}\")\n",
    "\n",
    "# Target: Recurred (Yes/No) → binary 1/0\n",
    "df['Recurred'] = (df['Recurred'] == 'Yes').astype(int)\n",
    "y = df['Recurred'].values\n",
    "X_raw = df.drop('Recurred', axis=1)\n",
    "\n",
    "# One-hot encode categorical columns (paper assumes numeric input)\n",
    "X = pd.get_dummies(X_raw, drop_first=True)\n",
    "print(f\"After one-hot: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36375019",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 2. Missing-data handling + conditioning check\n",
    "# ==============================================================\n",
    "# Paper: whole-batch conditioning is better than mini-batch.\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_filled = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "def batch_conditioning(X_df, batch_size=32):\n",
    "    conds = []\n",
    "    for i in range(0, len(X_df), batch_size):\n",
    "        batch = X_df.iloc[i:i+batch_size].values\n",
    "        if batch.shape[0] > 1 and np.linalg.matrix_rank(batch) > 0:\n",
    "            conds.append(condition(batch))\n",
    "    return np.mean(conds) if conds else np.inf\n",
    "\n",
    "mini_cond = batch_conditioning(X_filled, batch_size=32)\n",
    "whole_cond = condition(X_filled.values)\n",
    "print(f\"Mini-batch cond ≈ {mini_cond:,.1f} | Whole-batch cond ≈ {whole_cond:,.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f78320",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 3. Dimensionality reduction\n",
    "# ==============================================================\n",
    "# 3a. Inner-similarity: drop highly correlated features (|ρ|>0.8)\n",
    "corr = X_filled.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [c for c in upper.columns if any(upper[c] > 0.8)]\n",
    "X_inner = X_filled.drop(to_drop, axis=1)\n",
    "print(f\"Inner reduction → {X_inner.shape[1]} features (dropped {len(to_drop)})\")\n",
    "\n",
    "# 3b. Target-similarity: mutual information top-k\n",
    "mi = mutual_info_classif(X_inner, y, random_state=42)\n",
    "k = min(10, X_inner.shape[1])                 # keep at most 10\n",
    "top_idx = mi.argsort()[-k:][::-1]\n",
    "X_target = X_inner.iloc[:, top_idx]\n",
    "print(f\"Target reduction → {X_target.shape[1]} features\")\n",
    "\n",
    "X_reduced = X_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4da5ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 4. Size reduction – hierarchical clustering\n",
    "# ==============================================================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_reduced)\n",
    "\n",
    "Z = linkage(X_scaled, method='weighted', metric='euclidean')\n",
    "clusters = fcluster(Z, t=1.5, criterion='distance')   # tune t for ~30-50% reduction\n",
    "\n",
    "unique = np.unique(clusters)\n",
    "keep_idx = []\n",
    "for cid in unique:\n",
    "    mask = clusters == cid\n",
    "    centroid = X_scaled[mask].mean(axis=0)\n",
    "    dists = np.linalg.norm(X_scaled[mask] - centroid, axis=1)\n",
    "    keep_idx.append(np.where(mask)[0][dists.argmin()])\n",
    "\n",
    "X_final = X_reduced.iloc[keep_idx].reset_index(drop=True)\n",
    "y_final = y[keep_idx]\n",
    "print(f\"Size reduction → {X_final.shape[0]} samples (kept {len(keep_idx)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c0249",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 5. Train / test split\n",
    "# ==============================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final.values, y_final, test_size=0.2, stratify=y_final, random_state=42)\n",
    "\n",
    "# Torch tensors (float32)\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d0ae1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 6. ALGORITHM IMPLEMENTATIONS + MATHEMATICAL EXPLANATION\n",
    "# ==============================================================\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6.1 Batch Least Squares (BLS) – closed-form linear regression\n",
    "# ------------------------------------------------------------------\n",
    "\"\"\"\n",
    "BLS solves   y = X w + b   in the least-squares sense, then applies\n",
    "sigmoid for binary classification.\n",
    "\n",
    "Math:\n",
    "    w* = (XᵀX)⁻¹ Xᵀy          (Moore-Penrose if singular)\n",
    "    b* = mean(y - X w*)\n",
    "    ŷ  = σ(X w* + b*)         σ(z) = 1/(1+e⁻ᶻ)\n",
    "\"\"\"\n",
    "W_bls = pinv(X_train @ X_train.T) @ X_train @ y_train   # shape (n,1) → (d,1)\n",
    "b_bls = (y_train - X_train @ W_bls).mean()\n",
    "logits = X_test @ W_bls + b_bls\n",
    "y_pred_bls = (torch.sigmoid(torch.tensor(logits)) > 0.5).float()\n",
    "acc_bls = accuracy_score(y_test, y_pred_bls)\n",
    "print(f\"[BLS] Test accuracy: {acc_bls:.1%}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6.2 Iterative Neural Network (INN) – 1-layer perceptron with GD\n",
    "# ------------------------------------------------------------------\n",
    "\"\"\"\n",
    "A single linear layer + sigmoid, trained with stochastic gradient descent.\n",
    "Equivalent to logistic regression but implemented as a tiny NN.\n",
    "\n",
    "Loss:  L = BCE(σ(X w + b), y)\n",
    "Update: w ← w - η ∇L   (SGD)\n",
    "\"\"\"\n",
    "class INN(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim, 1)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "inn = INN(X_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(inn.parameters(), lr=0.05, momentum=0.9)\n",
    "\n",
    "for epoch in range(300):\n",
    "    optimizer.zero_grad()\n",
    "    out = inn(X_train_t)\n",
    "    loss = criterion(out, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_inn = (inn(X_test_t) > 0.5).float()\n",
    "acc_inn = accuracy_score(y_test_t, y_pred_inn)\n",
    "print(f\"[INN] Test accuracy: {acc_inn:.1%}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6.3 Least Squares with Linear Constraints (LSLC)\n",
    "# ------------------------------------------------------------------\n",
    "\"\"\"\n",
    "Same linear model as BLS but adds *non-negativity* on weights\n",
    "(and optionally L2 penalty). Paper uses a constrained optimizer.\n",
    "\n",
    "We implement a simple projected gradient descent:\n",
    "   w ← max(w - η ∇L, 0)\n",
    "\"\"\"\n",
    "class LSLC(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(dim, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x @ self.w + self.b)\n",
    "\n",
    "lslc = LSLC(X_train.shape[1])\n",
    "opt = optim.SGD(lslc.parameters(), lr=0.02)\n",
    "\n",
    "for epoch in range(400):\n",
    "    opt.zero_grad()\n",
    "    out = lslc(X_train_t)\n",
    "    loss = nn.BCELoss()(out, y_train_t)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # Projection: enforce w ≥ 0\n",
    "    with torch.no_grad():\n",
    "        lslc.w.clamp_(min=0.0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_lslc = (lslc(X_test_t) > 0.5).float()\n",
    "acc_lslc = accuracy_score(y_test_t, y_pred_lslc)\n",
    "print(f\"[LSLC] Test accuracy: {acc_lslc:.1%}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6.4 Imperialistic Competitive Algorithm (ICA) – population optimizer\n",
    "# ------------------------------------------------------------------\n",
    "\"\"\"\n",
    "Meta-heuristic inspired by socio-political competition.\n",
    "- Population = (countries) each holding a weight vector w + bias b\n",
    "- Imperialists = best countries\n",
    "- Colonies move toward their imperialist (assimilation)\n",
    "- Revolution & competition eliminate weak empires\n",
    "\n",
    "Fitness = 1 / (1 + MSE)   (higher → better)\n",
    "\"\"\"\n",
    "def sigmoid(z): return 1/(1+np.exp(-z))\n",
    "\n",
    "def mse_error(X, y, params):\n",
    "    w, b = params[:-1], params[-1]\n",
    "    pred = sigmoid(X @ w + b)\n",
    "    return np.mean((pred - y)**2)\n",
    "\n",
    "def ica_optimize(X, y, pop_size=60, max_iter=30, dim=X.shape[1]):\n",
    "    # ---- initialization ----\n",
    "    pop = np.random.uniform(-2, 2, (pop_size, dim+1))   # w + b\n",
    "    costs = np.array([mse_error(X, y, p) for p in pop])\n",
    "    \n",
    "    for it in range(max_iter):\n",
    "        # sort & assign imperialists (top 10%)\n",
    "        idx = np.argsort(costs)\n",
    "        pop, costs = pop[idx], costs[idx]\n",
    "        n_imp = max(1, pop_size//10)\n",
    "        imperialists = pop[:n_imp]\n",
    "        imp_costs    = costs[:n_imp]\n",
    "\n",
    "        # ---- assimilation ----\n",
    "        new_pop = pop.copy()\n",
    "        for i in range(n_imp, pop_size):\n",
    "            imp = imperialists[np.random.randint(n_imp)]\n",
    "            beta = np.random.uniform(0.1, 0.9)\n",
    "            new_pop[i] = pop[i] + beta * (imp - pop[i])\n",
    "            # small random revolution\n",
    "            if np.random.rand() < 0.05:\n",
    "                new_pop[i] += np.random.normal(0, 0.2, new_pop[i].shape)\n",
    "\n",
    "        # ---- evaluate new population ----\n",
    "        new_costs = np.array([mse_error(X, y, p) for p in new_pop])\n",
    "        \n",
    "        # ---- competition (replace worst imperialist if a colony is better) ----\n",
    "        for i in range(n_imp):\n",
    "            best_colony_idx = n_imp + np.argmin(new_costs[n_imp:])\n",
    "            if new_costs[best_colony_idx] < imp_costs[i]:\n",
    "                # swap\n",
    "                imperialists[i], new_pop[best_colony_idx] = new_pop[best_colony_idx], imperialists[i]\n",
    "                imp_costs[i], new_costs[best_colony_idx] = new_costs[best_colony_idx], imp_costs[i]\n",
    "\n",
    "        pop, costs = new_pop, new_costs\n",
    "\n",
    "    best = pop[np.argmin(costs)]\n",
    "    w_best, b_best = best[:-1], best[-1]\n",
    "    test_pred = sigmoid(X_test @ w_best + b_best)\n",
    "    y_pred_ica = (test_pred > 0.5).astype(float)\n",
    "    acc = accuracy_score(y_test, y_pred_ica)\n",
    "    print(f\"[ICA] Test accuracy: {acc:.1%}\")\n",
    "    return best\n",
    "\n",
    "ica_optimize(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda7dacd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 7. Summary of results\n",
    "# ==============================================================\n",
    "print(\"\\n=== FINAL SUMMARY ===\")\n",
    "print(f\"BLS   : {acc_bls:.1%}\")\n",
    "print(f\"INN   : {acc_inn:.1%}\")\n",
    "print(f\"LSLC  : {acc_lslc:.1%}\")\n",
    "# ICA printed inside function"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
